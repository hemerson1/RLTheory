{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dutch-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 20 Cost: 0.25\n",
      "---------------------\n",
      "Accuracy: 90.0\n",
      "Test Accuracy: 84.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 40 Cost: 0.14\n",
      "---------------------\n",
      "Accuracy: 97.5\n",
      "Test Accuracy: 84.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 60 Cost: 0.09\n",
      "---------------------\n",
      "Accuracy: 98.33\n",
      "Test Accuracy: 88.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 80 Cost: 0.07\n",
      "---------------------\n",
      "Accuracy: 96.67\n",
      "Test Accuracy: 88.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 100 Cost: 0.06\n",
      "---------------------\n",
      "Accuracy: 97.5\n",
      "Test Accuracy: 92.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 120 Cost: 0.05\n",
      "---------------------\n",
      "Accuracy: 97.5\n",
      "Test Accuracy: 92.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 140 Cost: 0.05\n",
      "---------------------\n",
      "Accuracy: 98.33\n",
      "Test Accuracy: 92.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 160 Cost: 0.04\n",
      "---------------------\n",
      "Accuracy: 98.33\n",
      "Test Accuracy: 92.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 180 Cost: 0.04\n",
      "---------------------\n",
      "Accuracy: 98.33\n",
      "Test Accuracy: 92.0\n",
      "---------------------\n",
      "\n",
      "\n",
      "Epoch: 200 Cost: 0.04\n",
      "---------------------\n",
      "Accuracy: 98.33\n",
      "Test Accuracy: 92.0\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Logistic Regression\n",
    "# Additional Implementation of L1 regularisation\n",
    "# Using the Iris dataset \n",
    "\n",
    "\"\"\"\n",
    "MODEL SUMMARY: \n",
    "- Loss function: Cross Entropy\n",
    "- Hidden function: ReLU\n",
    "- Output function: Softmax\n",
    "\n",
    "USER INFORMATION:\n",
    "- User can specify the neural network using Structure\n",
    "- Structure -> [input_dim, hidden layer_dims, ..., ouput_dim]\n",
    "\n",
    "NOTES:\n",
    "- Standardisation is not needed for logistic regression -> insensitive to scale\n",
    "- The derivative of the loss function with the softmax input is the same as the\n",
    "  derivative with the sigmoid input\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "A neural network using the relu activation function, softmax output and \n",
    "logisitic regression loss function.\n",
    "\"\"\"\n",
    "class Neural_Network:\n",
    "    def __init__(self, structure=[4, 10, 3], lr=0.1, epochs=200, up_freq=20, lamba=0.0, batch_size=10, epsilon=1e-7):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        self.units = structure\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.num_iterations = epochs\n",
    "        self.update_freq = up_freq\n",
    "        self.lamba = lamba\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.weights = []  \n",
    "        self.biases = []\n",
    "        weight = None \n",
    "        bias = None\n",
    "        \n",
    "        self.Z = []\n",
    "        self.A = []        \n",
    "        \n",
    "        # initialise the weights\n",
    "        for i in range(len(self.units) - 1):            \n",
    "            weight = np.random.randn(self.units[i + 1], self.units[i]) * (-1) \n",
    "            self.weights.append(weight)\n",
    "            \n",
    "            bias = np.zeros((self.units[i + 1], 1))\n",
    "            self.biases.append(bias) \n",
    "    \n",
    "    \"\"\"\n",
    "    Simple ReLU function\n",
    "    \"\"\"\n",
    "    def reLU(self, X):\n",
    "        return np.maximum(0, X) \n",
    "    \n",
    "    \"\"\"\n",
    "    Derivative of RelU function\n",
    "    \"\"\"    \n",
    "    def dx_reLU(self, X):\n",
    "        return np.where(X > 0.5, 1, 0)\n",
    "    \n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \"\"\"\n",
    "    def softmax(self, X): \n",
    "        return np.exp(X)/np.sum(np.exp(X), axis=0, keepdims=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the cost using the logistic regression cost function.\n",
    "    \"\"\"\n",
    "    def compute_cost(self, Y):        \n",
    "        m = Y.shape[0]        \n",
    "        return - 1/m * np.sum(Y * np.log(self.A[-1].T + self.epsilon))\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform a forward prediction with the model.\n",
    "    \"\"\"     \n",
    "    def feed_forward(self, X, update): \n",
    "        \n",
    "        a = X\n",
    "        \n",
    "        if update:\n",
    "            self.A = []\n",
    "            self.Z = [] \n",
    "        \n",
    "        # perform >= 1 hidden layer transformations\n",
    "        for i in range(len(self.units) - 2):\n",
    "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
    "            a = self.reLU(z) \n",
    "            \n",
    "            if update:\n",
    "                self.Z.append(z)\n",
    "                self.A.append(a)\n",
    "         \n",
    "        # perform softmax output transformation\n",
    "        z = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        a = self.softmax(z)\n",
    "        \n",
    "        if update:\n",
    "            self.Z.append(z)\n",
    "            self.A.append(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \"\"\"\n",
    "    Back propagate through the learned network.\n",
    "    \"\"\"  \n",
    "    def back_prop(self, X, Y):\n",
    "        \n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # compute derivative of cost and softmax\n",
    "        dz = self.A[-1] - Y\n",
    "        dw = (1/m) * np.dot(dz, self.A[-2].T)\n",
    "        db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "        self.weights[-1] = self.weights[-1] - self.lr * (dw + self.lamba * self.weights[-1]) \n",
    "        self.biases[-1] = self.biases[-1] - self.lr * (db + self.lamba * self.biases[-1])\n",
    "          \n",
    "        # cycle through hidden layers of NN \n",
    "        for i in range(len(self.units) - 3, -1, -1):\n",
    "            \n",
    "            if i == 0: \n",
    "                a = X.T\n",
    "            else: \n",
    "                a = self.A[i - 1].T\n",
    "                \n",
    "            dz = np.dot(self.weights[i+1].T, dz) * self.dx_reLU(self.A[i])\n",
    "            dw = (1/m) * np.dot(dz, a)\n",
    "            db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "            self.weights[i] = self.weights[i] - self.lr * (dw + self.lamba * self.weights[i]) \n",
    "            self.biases[i] = self.biases[i] - self.lr * (db + self.lamba * self.biases[i])        \n",
    "    \n",
    "    \"\"\"\n",
    "    Train the model on a sample of date.\n",
    "    \"\"\"   \n",
    "    def train(self, X_init, Y_init, X_test, Y_test):\n",
    "        \n",
    "        # determine the number of batchs\n",
    "        batch_int = math.floor(X_init.shape[1] / self.batch_size)\n",
    "        \n",
    "        for i in range(1, self.num_iterations + 1):\n",
    "            \n",
    "            # save the original\n",
    "            X_whole = X_init\n",
    "            Y_whole = Y_init\n",
    "            \n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            for j in range(1, batch_int + 1):   \n",
    "                \n",
    "                # create batchs\n",
    "                idx = list(range((j-1) * self.batch_size, j * self.batch_size))                  \n",
    "                X = X_whole[:, idx] \n",
    "                Y = Y_whole[:, idx]\n",
    "                \n",
    "                a = self.feed_forward(X, True)  \n",
    "                \n",
    "                # get prediction\n",
    "                Y_pred = np.zeros_like(Y.T)\n",
    "                Y_pred[np.arange(Y.shape[1]), a.T.argmax(1)] = 1\n",
    "                diff = Y.T.astype('int32') - Y_pred.astype('int32') \n",
    "                \n",
    "                # compute cost and accuracy \n",
    "                cost += self.compute_cost(Y.T)\n",
    "                accuracy += (1 - np.sum(np.sum(np.abs(diff), axis=1)/2)/Y.shape[1]) * 100    \n",
    "                \n",
    "                self.back_prop(X, Y)\n",
    "                    \n",
    "            if i % self.update_freq == 0: \n",
    "                \n",
    "                print('\\n\\nEpoch: {} Cost: {}'.format(i, round(cost/batch_int, 2)))        \n",
    "                print('---------------------')\n",
    "                print('Accuracy: {}'.format(round(accuracy/batch_int, 2)))   \n",
    "                \n",
    "                self.test(X_test, Y_test)       \n",
    "    \n",
    "    \"\"\"\n",
    "    Test the learned model against the true values.\n",
    "    \"\"\"   \n",
    "    def test(self, X, Y_true):\n",
    "        a = self.feed_forward(X, False) \n",
    "        \n",
    "        Y_pred = np.zeros_like(Y_true.T)\n",
    "        Y_pred[np.arange(Y_true.shape[1]), a.T.argmax(1)] = 1\n",
    "        diff = Y_true.T.astype('int32') - Y_pred.astype('int32')   \n",
    "       \n",
    "        print('Test Accuracy: {}'.format(round((1 - np.sum(np.sum(np.abs(diff), axis=1)/2)/Y_true.shape[1]) * 100, 2)))  \n",
    "        print('---------------------')\n",
    "        \n",
    "\"\"\"\n",
    "Convert the data into a usable form and then split the \n",
    "sample into training and testing.\n",
    "\"\"\"\n",
    "def pre_process(df):\n",
    "    \n",
    "    # remove NaN and shuffle\n",
    "    df = df.dropna()  \n",
    "    df = df.sample(frac=1) \n",
    "    \n",
    "    # convert the species names to numbers and save the mapping\n",
    "    df[\"species\"] = df[\"species\"].astype('category') \n",
    "    \n",
    "    # convert to array and one hot encode the \n",
    "    y = np.squeeze(np.array([pd.get_dummies(df['species']).to_numpy()])).T\n",
    "    \n",
    "    # standardise the dataset\n",
    "    x = df.loc[:, df.columns != 'species'].to_numpy().T\n",
    "    x_mean = np.mean(x, axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)\n",
    "    x = (x - x_mean)/x_std\n",
    "        \n",
    "    # split into test and training\n",
    "    y_train = y[:, 25:]\n",
    "    x_train = x[:, 25:]\n",
    "    y_test = y[:, :25]\n",
    "    x_test = x[:, :25]\n",
    "    \n",
    "    return y_train, x_train, y_test, x_test   \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # load and process the data\n",
    "    orig_train = pd.read_csv('./Data/iris_train.csv')\n",
    "    train = orig_train.copy()  \n",
    "\n",
    "    y_train, x_train, y_test, x_test = pre_process(train)\n",
    "        \n",
    "    # initialse the network\n",
    "    network = Neural_Network([4, 10, 3],\n",
    "                             lr=0.1, \n",
    "                             epochs=200,\n",
    "                             up_freq=20,\n",
    "                             lamba=0.0, \n",
    "                             batch_size=10,\n",
    "                             epsilon=1e-7\n",
    "                            )\n",
    "\n",
    "    # train and test\n",
    "    network.train(x_train, y_train, x_test, y_test)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
