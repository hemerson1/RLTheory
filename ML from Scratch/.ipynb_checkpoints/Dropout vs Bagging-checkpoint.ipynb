{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "decimal-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Network 1\n",
      "---------------------\n",
      "Epoch: 100 Cost: 0.68\n",
      "Accuracy: 61.68\n",
      "Epoch: 200 Cost: 0.66\n",
      "Accuracy: 80.24\n",
      "Epoch: 300 Cost: 0.65\n",
      "Accuracy: 85.03\n",
      "Epoch: 400 Cost: 0.65\n",
      "Accuracy: 94.61\n",
      "Epoch: 500 Cost: 0.64\n",
      "Accuracy: 99.4\n",
      "\n",
      "Training Network 2\n",
      "---------------------\n",
      "Epoch: 100 Cost: 0.67\n",
      "Accuracy: 61.45\n",
      "Epoch: 200 Cost: 0.67\n",
      "Accuracy: 57.23\n",
      "Epoch: 300 Cost: 0.67\n",
      "Accuracy: 57.23\n",
      "Epoch: 400 Cost: 0.68\n",
      "Accuracy: 57.23\n",
      "Epoch: 500 Cost: 0.68\n",
      "Accuracy: 57.23\n",
      "\n",
      "Training Network 3\n",
      "---------------------\n",
      "Epoch: 100 Cost: 0.64\n",
      "Accuracy: 60.84\n",
      "Epoch: 200 Cost: 0.62\n",
      "Accuracy: 92.17\n",
      "Epoch: 300 Cost: 0.62\n",
      "Accuracy: 97.59\n",
      "Epoch: 400 Cost: 0.62\n",
      "Accuracy: 98.8\n",
      "Epoch: 500 Cost: 0.62\n",
      "Accuracy: 96.99\n",
      "\n",
      "Training Network 4\n",
      "---------------------\n",
      "Epoch: 100 Cost: 0.64\n",
      "Accuracy: 61.21\n",
      "Epoch: 200 Cost: 0.63\n",
      "Accuracy: 89.7\n",
      "Epoch: 300 Cost: 0.63\n",
      "Accuracy: 92.73\n",
      "Epoch: 400 Cost: 0.63\n",
      "Accuracy: 64.85\n",
      "Epoch: 500 Cost: 0.61\n",
      "Accuracy: 96.36\n",
      "\n",
      "Training Summary\n",
      "---------------------\n",
      "Mean Cost: 0.64\n",
      "Mean Accuracy: 87.5\n",
      "---------------------\n",
      "\n",
      "Test Summary\n",
      "---------------------\n",
      "Test Accuracy: 95.2\n",
      "---------------------\n",
      "\n",
      "Epoch: 100 Cost: 0.67\n",
      "Accuracy: 59.34\n",
      "Epoch: 200 Cost: 0.66\n",
      "Accuracy: 63.25\n",
      "Epoch: 300 Cost: 0.64\n",
      "Accuracy: 88.25\n",
      "Epoch: 400 Cost: 0.64\n",
      "Accuracy: 72.89\n",
      "Epoch: 500 Cost: 0.64\n",
      "Accuracy: 96.08\n",
      "Epoch: 600 Cost: 0.63\n",
      "Accuracy: 100.0\n",
      "\n",
      "Test Summary:\n",
      "---------------------\n",
      "Test Accuracy: 98.0\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Comparing Two Forms of Regularisation: Bagging and Dropout \n",
    "# Using Binary Logistic Regression as a Basis \n",
    "# Using the Titanic dataset \n",
    "\n",
    "# MODEL SUMMARY: \n",
    "# Loss function: Cross Entropy\n",
    "# Hidden function: Sigmoid\n",
    "# Output function: Sigmoid\n",
    "\n",
    "# USER INFORMATION:\n",
    "# User can specify the neural network using Structure\n",
    "# Structure -> [input_dim, hidden layer_dims, ..., ouput_dim]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self, structure=[7,12,1], lr=0.1, epochs=100, up_freq=10, dropout_prob=1.0):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        self.units = structure\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.num_iterations = epochs\n",
    "        self.update_freq = up_freq\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        self.weights = []  \n",
    "        self.biases = []\n",
    "        \n",
    "        self.Z = []\n",
    "        self.A = []\n",
    "        \n",
    "        for i in range(len(self.units) - 1):\n",
    "            \n",
    "            # specifiying type removes overflow error\n",
    "            weight = np.random.rand(self.units[i + 1], self.units[i]).astype(np.float128)\n",
    "            self.weights.append(weight)\n",
    "            \n",
    "            bias = np.zeros((self.units[i + 1], 1))\n",
    "            self.biases.append(bias)       \n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        return 1/(1 + np.exp(-X))    \n",
    "    \n",
    "    # use logistic regression cost function\n",
    "    def compute_cost(self, Y):\n",
    "        m = Y.shape[1]        \n",
    "        cost_sum = np.multiply(np.log(self.A[-1]), Y) +  np.multiply((1 - Y), np.log(1 - self.A[-1]))\n",
    "        cost = np.squeeze(-np.sum(cost_sum) / m)\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def feed_forward(self, X, train=True):\n",
    "        \n",
    "        self.A = []\n",
    "        self.Z = [] \n",
    "        \n",
    "        z1 = np.dot(self.weights[0], X) + self.biases[0]\n",
    "        \n",
    "        # setting the dropout in the first layer to 0.9\n",
    "        # significantly improves performance\n",
    "        if train: \n",
    "            d = np.random.binomial(1, 0.9, size=z1.shape[0]) / 0.9           \n",
    "            z1 = (z1.T * d).T\n",
    "        \n",
    "        a = self.sigmoid(z1)         \n",
    "        self.Z.append(z1)\n",
    "        self.A.append(a)\n",
    "        \n",
    "        for i in range(len(self.units) - 2):            \n",
    "            \n",
    "            z = np.dot(self.weights[i+1], a) + self.biases[i+1]\n",
    "            \n",
    "            # ensure dropout is not applied to last layer\n",
    "            if train and i != len(self.units) - 3 and False:\n",
    "                d = np.random.binomial(1, self.dropout_prob, size=z.shape[0]) / self.dropout_prob \n",
    "                z = (z.T * d).T\n",
    "                \n",
    "            a = self.sigmoid(z)          \n",
    "            self.Z.append(z)\n",
    "            self.A.append(a) \n",
    "    \n",
    "    def back_prop(self, X, Y):\n",
    "        \n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # compute derivative of cost and sigmoid\n",
    "        dz = self.A[-1] - Y\n",
    "        dw = (1/m) * np.dot(dz, self.A[-2].T)\n",
    "        db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "        self.weights[-1] = self.weights[-1] - self.lr * dw\n",
    "        self.biases[-1] = self.biases[-1] - self.lr * db\n",
    "          \n",
    "        # cycle through hidden layers of NN \n",
    "        for i in range(len(self.units) - 3, 0, -1):\n",
    "            dz = np.dot(self.weights[i+1].T, dz) * self.sigmoid(self.A[i]) * (1 - self.sigmoid(self.A[i]))\n",
    "            dw = (1/m) * np.dot(dz, self.A[i - 1].T)\n",
    "            db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "            self.weights[i] = self.weights[i] - self.lr * dw\n",
    "            self.biases[i] = self.biases[i] - self.lr * db\n",
    "        \n",
    "        # compute output\n",
    "        dz = np.dot(self.weights[1].T, dz) * self.sigmoid(self.A[0]) * (1 - self.sigmoid(self.A[0]))         \n",
    "        dw = (1/m) * np.dot(dz, X.T)\n",
    "        db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "        \n",
    "        self.weights[0] = self.weights[0] - self.lr * dw\n",
    "        self.biases[0] = self.biases[0] - self.lr * db\n",
    "                    \n",
    "    def train(self, X, Y):\n",
    "        \n",
    "        cost, acc = 0, 0        \n",
    "        for i in range(1, self.num_iterations + 1):\n",
    "            self.feed_forward(X, train=True)  \n",
    "            \n",
    "            if i % self.update_freq == 0: \n",
    "                \n",
    "                Y_pred = np.around(self.A[-1], 0).astype(int)    \n",
    "                \n",
    "                cost = round(self.compute_cost(Y), 2)\n",
    "                acc = round((1 - np.abs(np.sum(Y - Y_pred)/Y.shape[1])) * 100, 2)                \n",
    "                print('Epoch: {} Cost: {}'.format(i, cost))\n",
    "                print('Accuracy: {}'.format(acc))                \n",
    "                \n",
    "            self.back_prop(X, Y)        \n",
    "                      \n",
    "        return cost, acc\n",
    "            \n",
    "    def test(self, X, Y_true):\n",
    "        self.feed_forward(X, train=False)          \n",
    "        Y_pred = np.around(self.A[-1], 0).astype(int)         \n",
    "        \n",
    "        print('\\nTest Summary:')\n",
    "        print('---------------------')\n",
    "        print('Test Accuracy: {}'.format(round((1 - np.abs(np.sum(Y_pred - Y_true)/Y_true.shape[1])) * 100, 2)))   \n",
    "        print('---------------------\\n')\n",
    "        \n",
    "    def test_return(self, X):\n",
    "        self.feed_forward(X, train=False)          \n",
    "        Y_pred = np.around(self.A[-1], 0).astype(int) \n",
    "        return Y_pred\n",
    "\n",
    "def pre_process(df):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # remove any unhelpful columns\n",
    "    del df['Name']\n",
    "    del df['Ticket']\n",
    "    del df['Cabin']\n",
    "    \n",
    "    # convert object columns to int or float columns\n",
    "    df['Sex'] = df['Sex'].eq('male').mul(1)\n",
    "    df['Embarked'] = pd.factorize(df['Embarked'])[0] + 1    \n",
    "    df = df.dropna()   \n",
    "    df = df.sample(frac=1)\n",
    "    \n",
    "    # convert to array    \n",
    "    y = np.array([df['Survived'].to_numpy()])\n",
    "    x = df.loc[:, df.columns != 'Survived'].to_numpy().T\n",
    "    \n",
    "    # split into test and training\n",
    "    y_train = y[:, 50:]\n",
    "    x_train = x[:, 50:]\n",
    "    y_test = y[:, :50]\n",
    "    x_test = x[:, :50]\n",
    "        \n",
    "    return y_train, x_train, y_test, x_test  \n",
    "\n",
    "class Bagging:\n",
    "    def __init__(self, structure=[7,12,1], lr=0.1, epochs=100, up_freq=100, dataset=None, num_networks=3):\n",
    "        \n",
    "        np.random.seed(0)        \n",
    "        self.num_networks = num_networks\n",
    "        [self.y_train, self.x_train, self.y_test, self.x_test] = dataset \n",
    "        \n",
    "        # divide up the dataset evenly        \n",
    "        whole_sample_split = int((self.y_train.shape[1] - self.y_train.shape[1] % num_networks) / num_networks)        \n",
    "        net_data_size = np.zeros(num_networks, dtype=np.int32) + whole_sample_split\n",
    "        \n",
    "        # add the leftover samples\n",
    "        for i in range(len(y_train) % num_networks):            \n",
    "            index = i % len(net_data_size)\n",
    "            net_data_size[index] += 1\n",
    "        \n",
    "        net_data_size = np.insert(net_data_size, 0, 0)        \n",
    "        self.net_data_size = np.cumsum(net_data_size)\n",
    "        \n",
    "        # initialise the neural networks\n",
    "        self.neural_networks = {}\n",
    "        for n in range(num_networks):\n",
    "            \n",
    "            network = Neural_Network(structure,\n",
    "                                     lr=lr,\n",
    "                                     epochs=epochs,\n",
    "                                     up_freq=up_freq,\n",
    "                                     dropout_prob=1.0\n",
    "                                    )\n",
    "            \n",
    "            self.neural_networks[\"network \" + str(n + 1)] = network\n",
    "            \n",
    "    def train(self):  \n",
    "        \n",
    "        running_cost = np.zeros(self.num_networks)\n",
    "        running_acc = np.zeros(self.num_networks)\n",
    "        \n",
    "        for n in range(self.num_networks):  \n",
    "            \n",
    "            print('\\nTraining Network {}'.format(n + 1))\n",
    "            print('---------------------')\n",
    "            \n",
    "            # get training examples\n",
    "            y_train = self.y_train[:, self.net_data_size[n]: self.net_data_size[n + 1]]\n",
    "            x_train = self.x_train[:, self.net_data_size[n]: self.net_data_size[n + 1]]\n",
    "            \n",
    "            # unpack the network and train the model\n",
    "            network = self.neural_networks[\"network \" + str(n + 1)]\n",
    "            cost, acc = network.train(x_train, y_train)  \n",
    "            \n",
    "            # save cost + accuracy\n",
    "            running_cost[n] = cost \n",
    "            running_acc[n] = acc\n",
    "            \n",
    "            # repack the network\n",
    "            self.neural_networks[\"network \" + str(n + 1)] = network     \n",
    "            \n",
    "        mean_cost = np.mean(running_cost)\n",
    "        mean_acc = np.mean(running_acc)\n",
    "        \n",
    "        print('\\nTraining Summary')\n",
    "        print('---------------------')\n",
    "        print('Mean Cost: {}'.format(round(mean_cost, 2)))\n",
    "        print('Mean Accuracy: {}'.format(round(mean_acc, 2)))\n",
    "        print('---------------------')\n",
    "    \n",
    "    def test(self):\n",
    "        \n",
    "        total_y_pred = np.zeros((self.num_networks, self.x_test.shape[1])) \n",
    "        x_test, y_test = self.x_test, self.y_test\n",
    "        \n",
    "        for n in range(self.num_networks):\n",
    "            \n",
    "            # unpack the network and test the model\n",
    "            network = self.neural_networks[\"network \" + str(n + 1)]\n",
    "            y_pred = network.test_return(x_test)     \n",
    "            \n",
    "            total_y_pred[n, :] = y_pred\n",
    "            \n",
    "        Y_pred = np.around(np.mean(total_y_pred, axis=0), decimals=1)\n",
    "        Y_true = y_test\n",
    "        \n",
    "        print('\\nTest Summary')\n",
    "        print('---------------------')\n",
    "        print('Test Accuracy: {}'.format(round((1 - np.abs(np.sum(Y_pred - Y_true)/Y_true.shape[1])) * 100, 2)))     \n",
    "        print('---------------------\\n')    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # load and process the data\n",
    "    orig_train = pd.read_csv('./titanic_train.csv', index_col=0)\n",
    "    train = orig_train.copy()       \n",
    "    y_train, x_train, y_test, x_test = pre_process(train)\n",
    "    \n",
    "    # initialse the bagging network\n",
    "    bagger = Bagging([7, 8, 12, 1],\n",
    "                    lr=0.08,\n",
    "                    epochs=500,\n",
    "                    up_freq=100,\n",
    "                    dataset=[y_train, x_train, y_test, x_test],\n",
    "                    num_networks=4\n",
    "                    )\n",
    "    \n",
    "    # train and test the bagging network\n",
    "    bagger.train()   \n",
    "    bagger.test()      \n",
    "    \n",
    "    # intialise the dropout network\n",
    "    network = Neural_Network([7, 8, 12, 1],\n",
    "                             lr=0.05,\n",
    "                             epochs=600,\n",
    "                             up_freq=100,\n",
    "                             dropout_prob=0.5                          \n",
    "                            )\n",
    "    \n",
    "    # train and test the dropout network\n",
    "    network.train(x_train, y_train)   \n",
    "    network.test(x_test, y_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
