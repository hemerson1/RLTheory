{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4287f6ed-7c91-46e7-8d41-a5c6693fa33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 50 - Loss: 0.197 - Val Loss:0.223 \n",
      "Ep: 100 - Loss: 0.164 - Val Loss:0.207 \n",
      "Ep: 150 - Loss: 0.153 - Val Loss:0.2 \n",
      "Ep: 200 - Loss: 0.146 - Val Loss:0.199 \n",
      "Ep: 250 - Loss: 0.139 - Val Loss:0.197 \n",
      "Ep: 300 - Loss: 0.133 - Val Loss:0.198 \n",
      "Ep: 350 - Loss: 0.13 - Val Loss:0.199 \n",
      "Ep: 400 - Loss: 0.125 - Val Loss:0.2 \n",
      "Ep: 450 - Loss: 0.122 - Val Loss:0.204 \n",
      "Ep: 500 - Loss: 0.124 - Val Loss:0.205 \n",
      "\n",
      "R2 Score: 0.8895672631318604\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "An Implementation of the method Neural Ordinary Differential \n",
    "Equation presented in: https://arxiv.org/abs/1806.07366\n",
    "\n",
    "\n",
    "TODO: \n",
    "\n",
    "- implement a neural ODE\n",
    "\n",
    "\n",
    "NOTES:\n",
    "- \n",
    "\n",
    "LINKS:\n",
    "\n",
    "- nice ref:\n",
    "https://github.com/msurtsukov/neural-ode/blob/master/Neural%20ODEs.ipynb\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Initialse the model parameters.\n",
    "\"\"\"\n",
    "def init_weights(layers, skips=[], scale=1.0, seed=0):\n",
    "    \n",
    "    # set the seed and create the weights\n",
    "    rng = np.random.RandomState(seed)\n",
    "    weights = [(scale * rng.randn(m, n), scale * rng.randn(n).reshape(1, -1)) for m, n in zip(layers[:-1], layers[1:])]\n",
    "    skip_weights = [None] * len(weights)    \n",
    "    \n",
    "    # create the skip weights\n",
    "    if len(skips) > 0:\n",
    "        skip_start, skip_stop = zip(*skips)\n",
    "        for idx in range(len(skips)):\n",
    "\n",
    "            # check the skips\n",
    "            cond1, cond2 = (skip_stop[idx] - skip_start[idx] > 1), (skip_start[idx] > 0)\n",
    "            cond3, cond4 = (skip_stop[idx] < len(weights)), (skip_stop[idx] < np.array(skip_start[idx + 1:])).all()\n",
    "            assert (cond1 & cond2 & cond3 & cond4), \"Invalid skip settings.\"\n",
    "\n",
    "            # add skip\n",
    "            n, m = weights[skip_start[idx]][0].shape[0], weights[skip_stop[idx] - 1][0].shape[1]\n",
    "            skip_weights[skip_stop[idx] - 1] = scale * rng.rand(n, m)\n",
    "    \n",
    "    return weights, skip_weights\n",
    "\n",
    "\"\"\"\n",
    "A basic residual neural network model set up so that \n",
    "skips are performed between layers of equal dimensions.\n",
    "\"\"\"\n",
    "class residual_NN:    \n",
    "    def __init__(self, layers, skips=[], seed=0):\n",
    "        \n",
    "        # intialise the parameters\n",
    "        self.weights, self.skip_weights = init_weights(layers, skips, scale=0.3, seed=seed)\n",
    "        self.skip_start, self.skip_stop = zip(*skips) if len(skips) > 0 else [], []\n",
    "        self.A = []\n",
    "        \n",
    "        # hyperparams\n",
    "        self.lr = 1e-3\n",
    "        self.lamba = 0.0\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the forward prediction of shape (batch_size, state_dim)\n",
    "    \"\"\"\n",
    "    def __call__(self, X):     \n",
    "        \n",
    "        A_log = [X]\n",
    "        skip_start_log, skip_end_log = [None] * len(self.weights), [None] * len(self.weights)\n",
    "        for idx, (w, b) in enumerate(self.weights):    \n",
    "                        \n",
    "            # linear + activation\n",
    "            Z = np.dot(X, w) + b        \n",
    "            A = np.tanh(Z) \n",
    "            \n",
    "            # log hidden states\n",
    "            A_log.append(A)\n",
    "            \n",
    "            # handle the skips layer\n",
    "            if (idx + 1) in self.skip_start: skip_start_log[idx] = A\n",
    "            if (idx + 1) in self.skip_stop: \n",
    "                start_idx = self.skip_start[self.skip_stop.index(idx + 1)]                   \n",
    "                A_skip = np.dot(skip_start_log[start_idx - 1], self.skip_weights[idx])\n",
    "                skip_end_log[idx] = A_skip\n",
    "                A += A_skip    \n",
    "            \n",
    "            # move to next layer\n",
    "            X = A\n",
    "        \n",
    "        # set intermediate states\n",
    "        self.A = A_log[:-1] + [Z]\n",
    "        self.A_skip_start, self.A_skip_end = skip_start_log, skip_end_log\n",
    "        \n",
    "        return Z   \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the model weights.\n",
    "    \"\"\"\n",
    "    def step(self, Y):\n",
    "        \n",
    "        skip_log = []\n",
    "        for idx, (w, b) in reversed(list(enumerate(self.weights))):\n",
    "            \n",
    "            # compute the cost function\n",
    "            N = Y.shape[0]\n",
    "            if idx == (len(self.weights) - 1):                \n",
    "                dz = np.sum(self.A[idx+1] - Y, axis=1, keepdims=True) \n",
    "                dw = (2/N) * np.dot(self.A[idx].T, dz) + (2/N) * (w * self.lamba)\n",
    "                db = (2/N) * np.sum(dz, axis=0)                \n",
    "                self.weights[idx] = (w - dw * self.lr, b - db * self.lr)                \n",
    "                continue\n",
    "            \n",
    "            # update the hidden layers               \n",
    "            dz_prev = dz\n",
    "            dz = (1 - np.square(np.tanh(self.A[idx+1]))) * np.dot(dz, self.weights[idx+1][0].T) \n",
    "            \n",
    "            # handle the skip\n",
    "            if (idx + 2) in self.skip_start: dz += skip_log[-1]\n",
    "            if (idx + 2) in self.skip_stop:                   \n",
    "                dz_s = np.dot(dz_prev, self.skip_weights[idx + 1].T)                                \n",
    "                dw_s = (1/N) * np.dot(self.A_skip_end[idx + 1].T, dz_s)                \n",
    "                curr_ws = self.skip_weights[idx + 1]\n",
    "                self.skip_weights[idx + 1] = curr_ws - (dw_s.T * self.lr)   \n",
    "                skip_log.append(dz_s)                \n",
    "            \n",
    "            dw = (1/N) * np.dot(self.A[idx].T, dz) + (2/N) * (w * self.lamba)         \n",
    "            db = (1/N) * np.sum(dz, axis=0)\n",
    "            self.weights[idx] = (w - dw * self.lr, b - db * self.lr)            \n",
    "            \n",
    "\"\"\"\n",
    "Simple Mean-Squared Error Loss\n",
    "\"\"\"\n",
    "def mse_loss(true, pred):   \n",
    "    return np.mean(np.sum(np.square(true - pred), axis=1))\n",
    "\n",
    "\"\"\"\n",
    "Calculate the R2 score for the model. \n",
    "\"\"\"\n",
    "def R2_score(true, pred):    \n",
    "    ssr = np.sum(np.square(pred - true))\n",
    "    sst = np.sum(np.square(pred - np.mean(true)))\n",
    "    return 1 - ssr/sst\n",
    "\n",
    "\"\"\"\n",
    "Run the training loop for the residual model.\n",
    "\"\"\"\n",
    "def train_model(model, dataset, loss_func, epochs=10, batch_size=32, seed=0):\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    train, val = dataset[0], dataset[1]    \n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # shuffle the datasets\n",
    "        rng.shuffle(train)\n",
    "        rng.shuffle(val)        \n",
    "        \n",
    "        losses = []\n",
    "        iters = int(len(train) // batch_size) + 1 \n",
    "        for it in range(iters):\n",
    "            \n",
    "            # get a batch of data            \n",
    "            x_tr = train[it * batch_size: min((it + 1) * batch_size, len(train)), :-1]            \n",
    "            y_tr = train[it * batch_size: min((it + 1) * batch_size, len(train)), -1]\n",
    "                        \n",
    "            # get the prediction            \n",
    "            y_pred = model(x_tr)\n",
    "        \n",
    "            # calculate the loss\n",
    "            reg_loss = model.lamba * np.sum([np.sum(np.square(w)) for w, _ in model.weights])\n",
    "            loss = loss_func(y_tr.reshape(-1, 1), y_pred) + reg_loss\n",
    "            \n",
    "            # track losses\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # update the weights \n",
    "            model.step(y_tr.reshape(-1, 1))\n",
    "            \n",
    "        # get validation loss\n",
    "        x_val, y_val = val[:, :-1], val[:, -1]            \n",
    "        y_val_pred = model(x_val)\n",
    "        reg_loss = model.lamba * np.sum([np.sum(np.square(w)) for w, _ in model.weights])\n",
    "        val_loss = loss_func(y_val.reshape(-1, 1), y_val_pred) + reg_loss\n",
    "        \n",
    "        # display loss\n",
    "        if (ep + 1) % (epochs // 10) == 0:\n",
    "            print('Ep: {} - Loss: {} - Val Loss:{} '.format(ep + 1, round(np.mean(losses), 3), round(val_loss, 3)))    \n",
    "\n",
    "\n",
    "# load in the dataset\n",
    "dataset = pd.read_csv(\"./Data/insurance_train.csv\")\n",
    "seed = 10000\n",
    "\n",
    "# convert columns to categorical\n",
    "dataset[\"sex\"] = dataset[\"sex\"].astype('category')\n",
    "dataset[\"region\"] = dataset[\"region\"].astype('category')\n",
    "dataset[\"smoker\"] = dataset[\"smoker\"].astype('category')\n",
    "\n",
    "# get the categorical columns\n",
    "cat_columns = dataset.select_dtypes(['category']).columns\n",
    "dataset[cat_columns] = dataset[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "# convert to a numpy array and normalise\n",
    "array = dataset.to_numpy()\n",
    "mean, std = np.mean(array, axis=0), np.std(array, axis=0)\n",
    "array = (array - mean) / std\n",
    "\n",
    "# shuffle array and get split\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(array)\n",
    "train_s, val_s, test_s = round(0.8 * len(array)), round(0.1 * len(array)), round(0.1 * len(array)) \n",
    "train, val, test = array[:train_s, :], array[train_s:(train_s+val_s), :], array[-test_s:, :]\n",
    "\n",
    "# NOTE: ---------------------------------------------------------------\n",
    "\n",
    "# layers: 60, 30, 20, 40\n",
    "# (1, 3) -> output of 60 node layer appended to output of 20 node layer \n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# instantiate the model\n",
    "x_dim, y_dim = train.shape[1] - 1, 1 \n",
    "model = residual_NN(\n",
    "    layers=[x_dim, 32, 32, 32, 32, 32, 32, 32, y_dim],\n",
    "    skips=[(1, 3), (4, 6)],\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# train the model\n",
    "train_model(\n",
    "    model=model,\n",
    "    dataset=(train, val),\n",
    "    loss_func=mse_loss,\n",
    "    batch_size=32,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "# test the accuracy of the model\n",
    "y_test, x_test = test[:, -1], test[:, :-1] \n",
    "y_pred = model(x_test)\n",
    "\n",
    "# unnorm the data\n",
    "y_pred = y_pred * std[-1] + mean[-1]\n",
    "y_test = y_test * std[-1] + mean[-1]\n",
    "\n",
    "print('\\nR2 Score: {}'.format(R2_score(pred=y_pred, true=y_test.reshape(-1, 1))))\n",
    "\n",
    "# plot the resutls\n",
    "plt.scatter(y_pred, y_test)\n",
    "plt.plot([-10_000, 60_000], [-10_000, 60_000])\n",
    "plt.xlim([-5_000, 50_000])\n",
    "plt.ylim([-5_000, 50_000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f22958-73fc-4f59-a5f3-9a2c2bd51b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# reference: https://github.com/msurtsukov/neural-ode/blob/master/Neural%20ODEs.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Initialse the model parameters.\n",
    "\"\"\"\n",
    "def init_weights(layers, scale=1.0, seed=0):\n",
    "    \n",
    "    # set the seed and create the weights\n",
    "    rng = np.random.RandomState(seed)\n",
    "    weights = [(scale * rng.randn(m, n), scale * rng.randn(n).reshape(1, -1)) for m, n in zip(layers[:-1], layers[1:])]\n",
    "            \n",
    "    return weights\n",
    "\n",
    "\"\"\"\n",
    "A simple feed-forward neural network model to \n",
    "approximate the derivative.\n",
    "\"\"\"\n",
    "class feedforward_NN:\n",
    "    def __init__(self, layers, scale=1.0, seed=0):        \n",
    "        self.weights = init_weights(layers, scale=scale, seed=seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the forward prediction of shape (batch_size, state_dim)\n",
    "    \"\"\"\n",
    "    def __call__(self, X, t):\n",
    "        \n",
    "        A_log = [X]\n",
    "        for idx, (w, b) in enumerate(self.weights):    \n",
    "                        \n",
    "            # linear + activation\n",
    "            Z = np.dot(X, w) + b        \n",
    "            A = np.max(Z, 0) \n",
    "            \n",
    "            # log hidden states\n",
    "            A_log.append(A)\n",
    "            \n",
    "            # move to next layer\n",
    "            X = A\n",
    "        \n",
    "        # set intermediate states\n",
    "        self.A = A_log[:-1] + [Z]\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\"\"\"\n",
    "Solve an ODE using the Euler Method.\n",
    "\"\"\"\n",
    "def euler_ode_solver(z0, t0, t1, f):\n",
    "    \n",
    "    # set the step size\n",
    "    dt_max = 0.05\n",
    "    n_max = np.max(np.abs(t1 - t0)/dt_max).astype(int)\n",
    "    dt = (t1 - t0)/n_max\n",
    "        \n",
    "    # recursively calculate z\n",
    "    z, t = z0, t0\n",
    "    for i in range(n_max):\n",
    "        z = z + dt * f(z, t)\n",
    "        t += dt\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "class ODEF(nn.Module):\n",
    "    \n",
    "    #########################\n",
    "        # TODO: clean this up!!!\n",
    "        #########################\n",
    "    \n",
    "    \n",
    "    def forward_with_grad(self, z, t, grad_outputs):\n",
    "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        out = self.forward(z, t)\n",
    "\n",
    "        a = grad_outputs\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        # grad method automatically sums gradients for batch items, we have to expand them back \n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters)\n",
    "\n",
    "\"\"\"\n",
    "Implementation of neural ordinary differential\n",
    "equation model.\n",
    "\"\"\"\n",
    "class neural_ode:\n",
    "    def __init__(self, layers, seed=0):\n",
    "        \n",
    "        # specify the functional approximator\n",
    "        self.func = feedforward_NN(\n",
    "            layers=layers, seed=seed\n",
    "        )\n",
    "        \n",
    "        # set the saved tensors\n",
    "    \n",
    "    def __call__(self, z0, t):\n",
    "                \n",
    "        # get the dimensions\n",
    "        batch_size, feat_num = z0.shape\n",
    "        t_len = t.shape[0]\n",
    "        \n",
    "        # store the forward pass\n",
    "        z = np.zeros((t_len, batch_size, feat_num))\n",
    "        z[0, :, :] = z0\n",
    "        \n",
    "        # solve for the hidden state through across the trajectory\n",
    "        for i_t in range(t_len - 1):\n",
    "            z0 = euler_ode_solver(z0, t[i_t], t[i_t+1], self.func)\n",
    "            z[i_t+1, :, :] = z0\n",
    "                \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def step(self, dLdz):\n",
    "        \n",
    "        #########################\n",
    "        # TODO: clean this up!!!\n",
    "        #########################\n",
    "        \n",
    "        func = ctx.func\n",
    "        t, z, flat_parameters = ctx.saved_tensors\n",
    "        time_len, bs, *z_shape = z.size()\n",
    "        n_dim = np.prod(z_shape)\n",
    "        n_params = flat_parameters.size(0)\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            tensors here are temporal slices\n",
    "            t_i - is tensor with size: bs, 1\n",
    "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
    "            \"\"\"\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim) \n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_len-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Pack augmented variable\n",
    "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients \n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None\n",
    "\n",
    "    \n",
    "    \n",
    "model = neural_ode(\n",
    "    layers=[1, 32, 32, 32, 1],\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "z = model(np.ones((32, 1)), np.arange(10))\n",
    "print(z.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
